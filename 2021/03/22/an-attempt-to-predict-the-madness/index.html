<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.81.0" />


<title>An attempt to predict the Madness - A Hugo website</title>
<meta property="og:title" content="An attempt to predict the Madness - A Hugo website">


  <link href='/favicon.ico' rel='icon' type='image/x-icon'/>



  








<link href='//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css' rel='stylesheet' type='text/css' />



<link rel="stylesheet" href="/css/fonts.css" media="all">
<link rel="stylesheet" href="/css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="/" class="nav-logo">
    <img src="/images/logo.png"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="/about/">About</a></li>
    
    <li><a href="https://github.com/MArtinherz">GitHub</a></li>
    
    <li><a href="https://twitter.com/martiiiinherz">Twitter</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">10 min read</span>
    

    <h1 class="article-title">An attempt to predict the Madness</h1>

    
    <span class="article-date">2021-03-22</span>
    

    <div class="article-content">
      


<p>This past weekend has not been kind to many brackets. My bracket right now ranks in the 72.5 percentile after the first weekend, a shock after being last after day one.</p>
<p>That doesn’t make me feel right but something is up when your bracket jumps from last to above the majority.</p>
<pre class="r"><code>library(tidyverse)</code></pre>
<pre><code>## -- Attaching packages --------------------------------------- tidyverse 1.3.0 --</code></pre>
<pre><code>## v ggplot2 3.3.2     v purrr   0.3.4
## v tibble  3.0.6     v dplyr   1.0.3
## v tidyr   1.1.2     v stringr 1.4.0
## v readr   1.3.1     v forcats 0.5.0</code></pre>
<pre><code>## Warning: package &#39;tibble&#39; was built under R version 4.0.3</code></pre>
<pre><code>## Warning: package &#39;tidyr&#39; was built under R version 4.0.3</code></pre>
<pre><code>## Warning: package &#39;dplyr&#39; was built under R version 4.0.3</code></pre>
<pre><code>## -- Conflicts ------------------------------------------ tidyverse_conflicts() --
## x dplyr::filter() masks stats::filter()
## x dplyr::lag()    masks stats::lag()</code></pre>
<pre class="r"><code>library(tidymodels)</code></pre>
<pre><code>## Warning: package &#39;tidymodels&#39; was built under R version 4.0.3</code></pre>
<pre><code>## -- Attaching packages -------------------------------------- tidymodels 0.1.2 --</code></pre>
<pre><code>## v broom     0.7.2      v recipes   0.1.15
## v dials     0.0.9      v rsample   0.0.8 
## v infer     0.5.4      v tune      0.1.2 
## v modeldata 0.1.0      v workflows 0.2.1 
## v parsnip   0.1.5      v yardstick 0.0.7</code></pre>
<pre><code>## Warning: package &#39;broom&#39; was built under R version 4.0.3</code></pre>
<pre><code>## Warning: package &#39;dials&#39; was built under R version 4.0.3</code></pre>
<pre><code>## Warning: package &#39;infer&#39; was built under R version 4.0.3</code></pre>
<pre><code>## Warning: package &#39;modeldata&#39; was built under R version 4.0.3</code></pre>
<pre><code>## Warning: package &#39;parsnip&#39; was built under R version 4.0.3</code></pre>
<pre><code>## Warning: package &#39;recipes&#39; was built under R version 4.0.3</code></pre>
<pre><code>## Warning: package &#39;rsample&#39; was built under R version 4.0.3</code></pre>
<pre><code>## Warning: package &#39;tune&#39; was built under R version 4.0.3</code></pre>
<pre><code>## Warning: package &#39;workflows&#39; was built under R version 4.0.3</code></pre>
<pre><code>## Warning: package &#39;yardstick&#39; was built under R version 4.0.3</code></pre>
<pre><code>## -- Conflicts ----------------------------------------- tidymodels_conflicts() --
## x scales::discard() masks purrr::discard()
## x dplyr::filter()   masks stats::filter()
## x recipes::fixed()  masks stringr::fixed()
## x dplyr::lag()      masks stats::lag()
## x yardstick::spec() masks readr::spec()
## x recipes::step()   masks stats::step()</code></pre>
<pre class="r"><code>library(zoo)</code></pre>
<pre><code>## Warning: package &#39;zoo&#39; was built under R version 4.0.3</code></pre>
<pre><code>## 
## Attaching package: &#39;zoo&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:base&#39;:
## 
##     as.Date, as.Date.numeric</code></pre>
<p>Let’s load in games data from my lovely class of all college basketball games over the last six seasons.</p>
<pre class="r"><code>games &lt;- read_csv(&#39;~/R/JOMC491/Data/updcbblogs1521.csv&#39;)</code></pre>
<pre><code>## Parsed with column specification:
## cols(
##   .default = col_double(),
##   Season = col_character(),
##   Date = col_date(format = &quot;&quot;),
##   TeamFull = col_character(),
##   Opponent = col_character(),
##   HomeAway = col_character(),
##   W_L = col_character(),
##   URL = col_character(),
##   Conference = col_character(),
##   Team = col_character()
## )</code></pre>
<pre><code>## See spec(...) for full column specifications.</code></pre>
<p>Creating the Metrics</p>
<p>I had a simple idea in generating my model. The four factors of basketball have become common to explain why and how a team does well. I wanted to capture that with a rolling average over the last 10 games and combining that to make a cumulative of all the rolling means as well.</p>
<pre class="r"><code>games &lt;- games %&gt;% mutate(
  Possessions = .5*(TeamFGA - TeamOffRebounds + TeamTurnovers + (.475 * TeamFTA)) + .5*(OpponentFGA - OpponentOffRebounds + OpponentTurnovers + (.475 * OpponentFTA)),
  OffensiveRating = (TeamScore/Possessions)*100, 
  DefensiveRating = (OpponentScore/Possessions)*100,
  TeamEFGRate = ((Team3P * .5) + TeamFG)/(TeamFGA), #how effectively we shoot from the field
  TeamFTRate = TeamFTA/Possessions, #how often we get to the line
  TeamTORate = TeamTurnovers/Possessions, #how often we turnover
  TeamOReboundRate = TeamOffRebounds/TeamTotalRebounds, #how well do we do the rebounds
) %&gt;%
  #By the Month
  group_by(Team, Season) %&gt;% #cumulative of our rolling mean
  mutate(RollyTeamEFGRate = rollmean(cummean(TeamEFGRate),k = 10, fill = TRUE, align = &#39;right&#39;), #how effectively we shoot from the field
         RollyTeamFTRate = rollmean(cummean(TeamFTRate), k = 10, fill = TRUE, align = &#39;right&#39;), #how often we get to the line
         RollyTeamTORate = rollmean(cummean(TeamTORate),  k = 10, fill = TRUE, align = &#39;right&#39;), #how often we turnover
         RollyTeamOReboundRate = rollmean(cummean(TeamOReboundRate),  k = 10, fill = TRUE, align = &#39;right&#39;), #how well do we do the rebounds
         RollyOffensiveRating = cummean(rollmean(OffensiveRating,  k = 10, fill = TRUE, align = &#39;right&#39;)), 
         RollyDefensiveRating = cummean(rollmean(DefensiveRating, k = 10, fill = TRUE, align = &#39;right&#39;)),
         RollyDifference = RollyOffensiveRating - RollyDefensiveRating) %&gt;%
  #our offense
  ungroup() %&gt;% 
  mutate(
    Location = case_when(
      str_trim(HomeAway) == &quot;@&quot; ~ &quot;A&quot;,
      str_trim(HomeAway) == &quot;N&quot; ~ &quot;N&quot;,
      TRUE ~ &quot;H&quot;
    ),
    Outcome = case_when(
      grepl(&quot;W&quot;, W_L) ~ &quot;W&quot;, 
      grepl(&quot;L&quot;, W_L) ~ &quot;L&quot;
    )
  ) %&gt;%
  mutate(Outcome = as.factor(Outcome))</code></pre>
<p>Our four factors were Team EFG% or team effective field goal rate, which puts equal weight on twos and threes to show how well a team shoots from the field with threes getting a 50% boost since those shots have 50% more value than a two. The next three were free throw rate or how often a team gets to the line, offensive rebound rate or how well our team does on the offensive glass and lastly our turnover rate.</p>
<p>Another addition is that we were taking the cumulative mean of our team and applying that to a rolling mean over the last 10 games so the model used recent performance to predict who would go furthest in the tournament.</p>
<pre class="r"><code>selectedgames &lt;- games %&gt;% select(Season, Team, Date, Opponent, Outcome, RollyTeamEFGRate, RollyTeamFTRate, RollyTeamTORate, RollyTeamOReboundRate, RollyOffensiveRating, RollyDefensiveRating, RollyDifference) %&gt;% na.omit()

opponentgames &lt;- selectedgames %&gt;% select(-Opponent) %&gt;% rename(Opponent = Team, RollyOppOffRating = RollyOffensiveRating, RollyOppDefRating = RollyDefensiveRating, RollyOppDifference = RollyDifference, RollyOpponentEFG = RollyTeamEFGRate, RollyOppoFTRate = RollyTeamFTRate, RollyOppoTORate = RollyTeamTORate, RollyOppoOReboundRAte = RollyTeamOReboundRate)

bothsides &lt;- games %&gt;% 
  left_join(opponentgames, by=c(&quot;Opponent&quot;, &quot;Season&quot;, &quot;Date&quot;)) %&gt;%
  rename(Outcome = Outcome.x) %&gt;%
  filter(!is.na(OpponentSRS))</code></pre>
<p>The idea here is pretty simple. Our match ups are going to be on the team’s four factors, offense only, along with their offensive and defensing rating. The difference is the +/- of a team per 100 possessions so teams with higher +/- are usually good on offense and defense. Sometimes, that can mean that one team is extremely good at one part such as Iowa on offense and average on the defensive end.</p>
<p>There rating is combined with the four factors to see if it adds to predicting wins or not.</p>
<p>The Model</p>
<p>The Training and Testing Data for our Model</p>
<pre class="r"><code>set.seed(1234)
log_split &lt;- initial_split(bothsides, prop = .8)

log_train &lt;- training(log_split) %&gt;% select(Team, Opponent, Date, Season, Outcome, RollyOffensiveRating, RollyOppOffRating, RollyOppDefRating , RollyDefensiveRating, RollyOppDifference , RollyDifference, RollyOpponentEFG , RollyTeamEFGRate, RollyOppoFTRate , RollyTeamFTRate, RollyOppoTORate , RollyTeamTORate, RollyOppoOReboundRAte , RollyTeamOReboundRate ) %&gt;% na.omit()

log_test &lt;- testing(log_split) %&gt;% select(Team, Opponent, Date, Season, Outcome, RollyOffensiveRating, RollyOppOffRating, RollyOppDefRating , RollyDefensiveRating, RollyOppDifference , RollyDifference, RollyOpponentEFG , RollyTeamEFGRate, RollyOppoFTRate , RollyTeamFTRate, RollyOppoTORate , RollyTeamTORate, RollyOppoOReboundRAte , RollyTeamOReboundRate ) %&gt;% na.omit()</code></pre>
<p>The Model</p>
<pre class="r"><code>log_recipe &lt;- 
  recipe(Outcome ~ ., data = log_train) %&gt;% 
  update_role(Team, Opponent, Date, Season, new_role = &quot;ID&quot;) %&gt;%
  step_dummy(all_nominal(), -all_outcomes()) %&gt;% 
  step_normalize(all_predictors())

summary(log_recipe)</code></pre>
<pre><code>## # A tibble: 19 x 4
##    variable              type    role      source  
##    &lt;chr&gt;                 &lt;chr&gt;   &lt;chr&gt;     &lt;chr&gt;   
##  1 Team                  nominal ID        original
##  2 Opponent              nominal ID        original
##  3 Date                  date    ID        original
##  4 Season                nominal ID        original
##  5 RollyOffensiveRating  numeric predictor original
##  6 RollyOppOffRating     numeric predictor original
##  7 RollyOppDefRating     numeric predictor original
##  8 RollyDefensiveRating  numeric predictor original
##  9 RollyOppDifference    numeric predictor original
## 10 RollyDifference       numeric predictor original
## 11 RollyOpponentEFG      numeric predictor original
## 12 RollyTeamEFGRate      numeric predictor original
## 13 RollyOppoFTRate       numeric predictor original
## 14 RollyTeamFTRate       numeric predictor original
## 15 RollyOppoTORate       numeric predictor original
## 16 RollyTeamTORate       numeric predictor original
## 17 RollyOppoOReboundRAte numeric predictor original
## 18 RollyTeamOReboundRate numeric predictor original
## 19 Outcome               nominal outcome   original</code></pre>
<p>This is our recipe for the model where we predict the outcome with our chosen metrics and normalize all numbers to be on the same level.</p>
<pre class="r"><code>log_mod &lt;- 
  rand_forest() %&gt;% 
  set_engine(&quot;ranger&quot;) %&gt;%
  set_mode(&quot;classification&quot;)

log_workflow &lt;- 
  workflow() %&gt;% 
  add_model(log_mod) %&gt;% 
  add_recipe(log_recipe)</code></pre>
<p>After that, our model uses random forests to give a probability on game outcomes with W or L. I chose random forests because the model tries to pick decision trees that aren’t correlated with one another and the model is not as mean to my computer.</p>
<p>That creates our workflow before creating the model.</p>
<pre class="r"><code>log_fit &lt;- 
  log_workflow %&gt;% 
  fit(data = log_train)</code></pre>
<p>In three lines of code, our model is created.</p>
<pre class="r"><code>trainpredict &lt;- log_fit %&gt;% predict(new_data = log_train) %&gt;%
  bind_cols(log_train)

metrics(trainpredict, Outcome, .pred_class)</code></pre>
<pre><code>## # A tibble: 2 x 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy binary         0.947
## 2 kap      binary         0.893</code></pre>
<p>93% is pretty damn high, a little too high even and that number goes down. By a lot.</p>
<pre class="r"><code>testpredict &lt;- log_fit %&gt;% predict(new_data = log_test) %&gt;%
  bind_cols(log_test)


metrics(testpredict, Outcome, .pred_class)</code></pre>
<pre><code>## # A tibble: 2 x 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy binary         0.661
## 2 kap      binary         0.321</code></pre>
<p>The shortcomings within this model stem from how overfitted the model is compared to the test data. Our model’s accuracy with the training data is in the lower 90s at 93 % for accuracy. If the model was this accurate, I would have my best bracket performance ever.</p>
<p>Instead, the model accuracy drops to about 69%. That’s still pretty good in predicting outcomes of games but shows that there are prominent issues with this model.</p>
<p>There’s a very high chance that one metric is correlated really strongly with any metric but I continued on to see what happens next. Anyways, we continued into the final part and that is the simulation.</p>
<p>The model was put to the test day one with the South region.</p>
<pre class="r"><code>south &lt;- tibble(
  Team=&quot;Baylor&quot;,
  Opponent=&quot;Hartford&quot;,
  Date = as.Date(&quot;2021-03-19&quot;)
) %&gt;% add_row(
  Team=&quot;North Carolina&quot;,
  Opponent=&quot;Wisconsin&quot;,
  Date = as.Date(&quot;2021-03-19&quot;)
) %&gt;% add_row(
  Team=&quot;Villanova&quot;,
  Opponent=&quot;Winthrop&quot;,
  Date = as.Date(&quot;2021-03-19&quot;)
) %&gt;% add_row(
  Team=&quot;Purdue&quot;,
  Opponent=&quot;North Texas&quot;,
  Date = as.Date(&quot;2021-03-19&quot;)
) %&gt;% add_row(
  Team=&quot;Texas Tech&quot;,
  Opponent=&quot;Utah State&quot;,
  Date = as.Date(&quot;2021-03-19&quot;)
) %&gt;% add_row(
  Team=&quot;Arkansas&quot;,
  Opponent=&quot;Colgate&quot;,
  Date = as.Date(&quot;2021-03-19&quot;)
) %&gt;% add_row(
  Team=&quot;Florida&quot;,
  Opponent=&quot;Virginia Tech&quot;,
  Date = as.Date(&quot;2021-03-19&quot;)
) %&gt;% add_row(
  Team=&quot;Ohio State&quot;,
  Opponent=&quot;Oral Roberts&quot;,
  Date = as.Date(&quot;2021-03-19&quot;)
) </code></pre>
<pre class="r"><code>south &lt;- selectedgames %&gt;% group_by(Team) %&gt;% filter(Date == max(Date), Season == &quot;2020-2021&quot;) %&gt;% select(-Date, -Opponent, -Outcome) %&gt;% right_join(south)</code></pre>
<pre><code>## Joining, by = &quot;Team&quot;</code></pre>
<pre class="r"><code>south &lt;- opponentgames %&gt;% group_by(Opponent) %&gt;% filter(Date == max(Date)) %&gt;% ungroup()  %&gt;% select(-Season, -Date, -Outcome) %&gt;% right_join(south, by=c(&quot;Opponent&quot;)) %&gt;% select(Team, everything())

southregional &lt;- log_fit %&gt;% predict(new_data = south) %&gt;%
  bind_cols(south) 

southregional &lt;- log_fit %&gt;% predict(new_data = southregional, type=&quot;prob&quot;) %&gt;%
  bind_cols(southregional)

southregional</code></pre>
<pre><code>## # A tibble: 8 x 21
##   .pred_L .pred_W .pred_class Team  Opponent RollyOpponentEFG RollyOppoFTRate
##     &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;       &lt;chr&gt; &lt;chr&gt;               &lt;dbl&gt;           &lt;dbl&gt;
## 1   0.495   0.505 W           Arka~ Colgate             0.579           0.260
## 2   0.263   0.737 W           Bayl~ Hartford            0.497           0.229
## 3   0.534   0.466 L           Purd~ North T~            0.558           0.221
## 4   0.430   0.570 W           Ohio~ Oral Ro~            0.543           0.223
## 5   0.582   0.418 L           Texa~ Utah St~            0.508           0.260
## 6   0.560   0.440 L           Flor~ Virgini~            0.523           0.292
## 7   0.459   0.541 W           Vill~ Winthrop            0.533           0.297
## 8   0.559   0.441 L           Nort~ Wiscons~            0.504           0.243
## # ... with 14 more variables: RollyOppoTORate &lt;dbl&gt;,
## #   RollyOppoOReboundRAte &lt;dbl&gt;, RollyOppOffRating &lt;dbl&gt;,
## #   RollyOppDefRating &lt;dbl&gt;, RollyOppDifference &lt;dbl&gt;, Season &lt;chr&gt;,
## #   RollyTeamEFGRate &lt;dbl&gt;, RollyTeamFTRate &lt;dbl&gt;, RollyTeamTORate &lt;dbl&gt;,
## #   RollyTeamOReboundRate &lt;dbl&gt;, RollyOffensiveRating &lt;dbl&gt;,
## #   RollyDefensiveRating &lt;dbl&gt;, RollyDifference &lt;dbl&gt;, Date &lt;date&gt;</code></pre>
<p>The South Region day one games all had probabilities between 40 to 60 percent for a win or loss except for Baylor. The model predicted Baylor would win with 73 percent confidence. One upset the model got right was North Texas shocking Purdue while also somewhat thinking that Oral Roberts had a reasonable chance at beating the Buckeyes.</p>
<p>The model does some things right and does some things wrong. There is an obvious emphasis on offensive stats only as the model predicts who wins only on offense and the use of ratings helps muddy up the waters a bit.</p>
<p>Still, there is legitimacy to using these numbers as the four factors, first used by Dean Oliver, are still good predicting at the final outcome of games.</p>
<p>Now, my current bracket.</p>
<p>Three of my Final Four teams are still alive with Gonzaga, Baylor and Loyola-Chicago.</p>
<p>The national champion: Gonzaga over Baylor with a 59% chance at winning that game.</p>
<p><img src="images/TopHalf.png" /></p>
<p><img src="images/BottomHalf.png" /></p>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="/index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="/images/hugo-logo.png" alt="Img link to Hugo website" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/r.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/yaml.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



    

    
  </body>
</html>

